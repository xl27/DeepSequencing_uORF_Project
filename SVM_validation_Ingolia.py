##################################################
#####        Import Relevant Packages        #####
##################################################

#Basic ribosome profiling packages
from __future__ import division
from plastid import GTF2_TranscriptAssembler, BAMGenomeArray, VariableFivePrimeMapFactory, GenomeHash
import twobitreader as twobit
import numpy as np
import re
from collections import Counter, defaultdict
import itertools
from plastid.util.io.filters import CommentReader

#Specific packages for this script
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import more_itertools as mit
import dill
from pathos.multiprocessing import ProcessingPool as Pool
from copy import deepcopy

#Machine Learning
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

##################################################
#####     Functions to Process CHX Data      #####
##################################################
def count_CHXreads_to_dataframe(list_of_transcripts, path_to_CHX_reads, path_to_CHX_psite, nthreads):
    """A multithreaded function which returns a pandas dataframe containing a plastid transcript object and its CHX read counts, CDS lengths, rpnt for a list of plastid transcript objects. This is helpful for percentile-based subsetting of transcript list for SVM training. 
    
    --Inputs--
    list_of_transcripts: list of plastid transcript objects. All objects should have a CDS annotated, as we will count CHX reads in the CDS to approximate translation
    path_to_CHX_reads: string giving path to .bam file for CHX read data in the tissue of interest
    path_to_CHX_psite: string giving path to file containing psite offsets (generated by plastid psite script) for transcripts of interest. 
    
    --Output--
    Pandas dataframe of dimension (n input transcript, 4) with columns ['segmentchain','cds_reads','length','rpnt'] which are self-explanatory. Segmentchain column contains actual transcript objects to facilitate out-of-the-box application as an iterable for downstream work (ie. no need to look up the transcripts after subsetting).
    """
    def count_CHX_single_transcript(input_transcript, path_to_CHX_reads, path_to_CHX_psite):
        print 'Working On ' + str(input_transcript.get_name()) + '...'
        
        #Prepare reads and transcript object
        CHX_reads = BAMGenomeArray(path_to_CHX_reads)
        CHX_reads.set_mapping(VariableFivePrimeMapFactory.from_file(open(path_to_CHX_psite)))
        cds_transcript=input_transcript.get_cds()
        
        #Count features
        counts=np.nansum(cds_transcript.get_counts(CHX_reads))
        length=cds_transcript.get_length()
        rpnt_cds=counts/length
        print '...Done!'
        return input_transcript, counts, length, rpnt_cds
    
    output = Pool(nthreads).map(count_CHX_single_transcript, list_of_transcripts, itertools.repeat(path_to_CHX_reads), itertools.repeat(path_to_CHX_psite))
    
    return pd.DataFrame(output, columns = ['segmentchain','cds_reads','length','rpnt'])


##################################################
##### Functions to Construct Ingolia Vectors #####
##################################################

def get_codon_count_from_relative_position(start_codon_absolute,rel_codon_pos, count_vector):
    """Get a codon read count (-1, 0, +1), when codon is specified as a relative position from a start codon, whose absolute position in transcript coordinates is known.
    
    Requires location of the start codon (i.cds_start attribute if a known start codon), relative codon position and a count vector for that transcript (i.get_counts(my_BAMgenomearray))
    
    --Input--
    start_codon_absolute: fixed point on transcript (nt, in transcript coordinates) from which relative position will be defined. For example, when constructing the Ingolia vector, this would be the transcript coordinate for the cds_start (ie. i.cds_start attribute of plastid transcript object). 
    rel_codon_position: distance in CODONS from fixed point to codon (-1,0,1) where read count is desired
    count_vector: a vector of counts at each position of transcript being used to construct vectors. Use object.get_counts(BAM array) to generate.
    
    --Output--
    An integer with the read count at the desired codon position, relative to a fixed starting point
     
    """
    
    codon_position_in_nt= start_codon_absolute + (rel_codon_pos * 3)
    final_count=np.nansum(count_vector[codon_position_in_nt-1:codon_position_in_nt+2]) #get codon score -1,-,+1 inclusive.
    return final_count

def construct_Ingolia_vector(site_to_zero, count_vector):
    """Construct an 8 item vector of codon-wise read counts as per Ingolia 2011. 
    Codon positions summed as [[−2, −1], [0], [1], [2], [3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13]] relative to candidate codon at position 0, reads taken at nt -1,0,1 for each codon. 
    
    --Input--
    site_to_zero: position, specified in transcript coordinates, around which the Ingolia vector should be constructed. This function will take that position, use 'get_codon_count_from_relative_position' to count reads at proper codon locations relative to the position, and return the vector
    count_vector: passed to get_codon_count_from_relative_position. See description there.
    
    --Output--
    A vector of length 8 as specified by Ingolia 2011 Cell for given nucleotide position, site_to_zero
    
    """
    codon_vector=[[-2, -1], [0], [1], [2], [3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13]]
    final_vector=[]
    for position in codon_vector:
        position_counts=map(lambda iter: get_codon_count_from_relative_position(site_to_zero,iter,count_vector), position)
        vector_count=sum(position_counts)
        final_vector.append(vector_count)
    
    return final_vector    

##########################################################
##### Functions to Construct Arrays for Training SVM #####
##########################################################

def generate_SVM_arrays_around_cds_start(list_input_transcripts, path_to_harringtonine_reads, path_to_harringtonine_psite, nthreads):
    """Generates positive and negative vectors for training SVM to predict start peaks using Harringtonine ribosome profiling data. 
        (1) Loops through list of input transcripts, generating positive and negative example vectors from each transcript
            (a) Using the annotated_CDS_start for each transcript as a positive example, constructs the Ingolia vector 
            (b) Constructs Ingolia vectors for 10 negative locations on each transcript as specified in 2011 cell paper 
        (2) Concatenates positive and negative vectors into a single numpy array which is output
        (3) Returns a second array with labels for each vector (1=positive example, 0=negative example)
        
        --Input--
        list_input_transcripts: iterable containing plastid transcript objects to be processed. All supplied transcripts are processed, so be sure you have filtered for highly expressed transcripts, and split into test/training groups beforehand.
        path_to_harringtonine_reads: path to .bam file containing harringtonine reads used to construct the arrays
        path_to_harringtonine_psite: path to file generated by plastid psite script giving psite offsets for harringtonine reads.
        nthreads: number cores to use for parallel processing. Please note that this uses pathos.multiprocessing ProcessingPool and it expects to find 'Pool' defined globally. You MUST run 'from pathos.multiprocessing import ProcessingPool as Pool' before trying this function!
        
        --Output--
        Two np arrays: the first contains Ingolia vectors constructed from the input list. The second contains assignments (1=positive example, ie. vector constructed from annotated CDS_start locations. 0=negative example, constructed as specified in 2011 Cell paper). There are 10 negative examples per 1 positive example on each of the input transcripts passing basic QC filtering.
    """
    def process_single_transcript_forSVM(input_transcript, path_to_harringtonine_reads, path_to_harringtonine_psite):
        print 'Working on ' + input_transcript.get_name() + '...'
        #Set up harringtonine reads
        harringtonine_reads = BAMGenomeArray(path_to_harringtonine_reads)
        harringtonine_reads.set_mapping(VariableFivePrimeMapFactory.from_file(open(path_to_harringtonine_psite)))
        
        #Set up vectors to append into
        positive_vectors=[]
        negative_vectors=[]
        
        #Ensure transcript competant to be a test example
        start_codon_nt = input_transcript.cds_start
        if not start_codon_nt-25 > 0 or not start_codon_nt+190 < input_transcript.get_length():#Ingolia says -18 on each side of initiation site scoring window. Noting that scoring window is -7 to 40 nt from given site, accounting for negative vector should be 150+40=190 on positive bound.
            return positive_vectors, negative_vectors
        
        #Create the vectors
        count_vector=input_transcript.get_counts(harringtonine_reads)
        positive_vectors.append(construct_Ingolia_vector(start_codon_nt,count_vector))
        for z in [-6,-3,3,9,18,30,60,90,120,150]:
            negative_vectors.append(construct_Ingolia_vector(start_codon_nt+z,count_vector))
        #
        print '...Done!'
        return positive_vectors, negative_vectors
    #
    output = Pool(nthreads).map(process_single_transcript_forSVM, list_input_transcripts, itertools.repeat(path_to_harringtonine_reads), itertools.repeat(path_to_harringtonine_psite))
    
    #Unpack mapped output. Each iteration is output as a list, which contains two sub-lists with positive and negative vectors
    positive_vectors = [x[0] for x in output if len(x[0])>0]
    negative_vectors = [x[1] for x in output if len(x[0])>0] #if x[0] = [], x[1] will also = []. This just removes iterations where the transcript didn't pass QC
    
    #Second level of unpacking. Take list of lists of vectors to just a list of vectors
    positive_vectors = list(itertools.chain.from_iterable(positive_vectors))
    negative_vectors = list(itertools.chain.from_iterable(negative_vectors))
    
    ingolia_vector_array=np.asarray(positive_vectors+negative_vectors)   
    identities_for_array=np.asarray([1]*len(positive_vectors) + [0]*len(negative_vectors))
    #
    return ingolia_vector_array, identities_for_array

################################################################################
##### Functions using Trained SVM to predict uORFs from Harringtonine Data #####
################################################################################

def tile_SVM_utr5(segment_chain,count_vector,trained_SVM, scale_logical, scaler_function):
    """Construct Ingolia vectors (by calling construct_Ingolia_vector) for all positions in 5'UTR of input segment chain, predicting whether each site is a start codon using a trained SVM and returning only the positive hits as a list
    
    --Input--
    segment_chain: plastid 'transcript' object to be used for analysis
    count_vector: vector of counts in harringtonine dataset at all positions in segment_chain (segment_chain.get_counts(BAM array Harring))
    trained_SVM: an SVC object (from sklearn.svm) which has been trained on annotated CDS as per Ingolia
    scale_logical: boolean, should the ingolia vector be scaled (ie. if the SVM was trained on a scaled array)
    scaler_function: function of type preprocessing.StandardScaler().fit(training_array) for given training_array. Required if the SVM was trained on a scaled array (ie. if scale_logical=True)
    
    --Output--
    List of nt positions (in transcript coordinates) where Ingolia vector was classified as 'positive' by the trained_SVM
    If no positive positions or a UTR <7nt, returns an empty list [] 
    """
    #
    positive_call=[]
    if segment_chain.get_utr5().get_length() < 7:
        return positive_call
    #
    for z in range(7,segment_chain.cds_start-1):
        if (z+40 < segment_chain.get_length()):
            ingolia_vector=np.asarray(construct_Ingolia_vector(z,count_vector))
            if scale_logical:
                ingolia_vector = ingolia_vector.reshape(1, -1)
                ingolia_vector = scaler_function.transform(ingolia_vector)
                if trained_SVM.predict(ingolia_vector)[0]==1:
                    positive_call.append(z)
                #
            #
        elif trained_SVM.predict(ingolia_vector.reshape(1,-1))[0]==1:
                positive_call.append(z)
            #
        #
    #
    return positive_call

def find_start_in_peak(peak_range, transcript_sequence, count_vector, reads_cutoff, canonical_start_codon, nearcanonical_start_codon):
    """Takes a harringtonine peak, defined as a list of nt positions (converted to np.array internally) predicted by SVM. Implements peak QC filtering before looking for a start codon and returning a segmentchain spanning the identified ORF. Canonical start codons are given preference. 
    
    --Input--
    peak_range: list of nt positions (transcript coordinates) passing SVM prediction
    transcript_sequence: nt sequence of the transcript
    count_vector: array of counts for transcript of interest
    reads_cutoff: minimum number of reads in the peak's 'vecotor window' (-7, +40 relative to the peak boundaries) for QC filtering
    canonical_start_codon, nearcanonical_start_codon, stop_codon: regex (specified as re.compile(regex)) for indicated codons
    **note canonical start codons are given preference in the script.
    
    --Output--
    Integer of type 'numpy.int64' containing first nt of start codon, if start codon can be found in peak
    If no start codon can be found, returns type(str) with explanation.
    
    """
    
    #Convert Peak to Numpy Array for Better Performance
    range_numpy=np.asarray(peak_range)
    #Initial QC
    if np.nansum(count_vector[min(range_numpy)-7:max(range_numpy)+40]) < reads_cutoff:
        return 'none. failed QC'
    
    search_space_sequence=transcript_sequence[min(range_numpy)-7:max(range_numpy)+40]
    
    #Find Canonical Starts, restricted to the peak boundaries. Convert to numpy array
    possible_canonical_starts=[]
    for location in canonical_start_codon.finditer(search_space_sequence):
        possible_canonical_starts.append(location.start())
    
    possible_canonical_starts=np.asarray(possible_canonical_starts)
    possible_canonical_starts=possible_canonical_starts + min(range_numpy) - 7 #starts are relative to search space. We want transcript coords
    
    #Check if there are canonical starts. 
    if len(possible_canonical_starts)>0:
        #If so, find the most likely, assuming this is closest to the center of the harringtonine peak
        likely_start = possible_canonical_starts[np.argmin(np.absolute(possible_canonical_starts-(sum(peak_range)/len(peak_range))))]
    
    else:
        #If not, check for non-canonical restricted to peak boundaries
        possible_noncanonical_starts=[]
        for location in nearcanonical_start_codon.finditer(search_space_sequence):
            possible_noncanonical_starts.append(location.start())
        
        possible_noncanonical_starts=np.asarray(possible_noncanonical_starts)
        possible_noncanonical_starts=possible_noncanonical_starts + min(range_numpy) - 7 #starts are relative to search space. We want transcript coords
        
        if len(possible_noncanonical_starts)>0:
            likely_start = possible_noncanonical_starts[np.argmin(np.absolute(possible_noncanonical_starts-(sum(peak_range)/len(peak_range))))]
        
        else:
            likely_start = 'none'
        #
    #
    return likely_start

def pair_start_with_stop(start_codon_array, transcript_sequence, stop_codon_regex):
    """Takes a list of start codons for a given sequence and pairs with first in-frame stop codon. Does NOT check that this is not the CDS_end for the sake of flexiability (preserves ability to call N-terminal extensions). 
    
    --Input--
    start_codon_array: array (internally coerced to np.array) containing start codons identified in a sequence
    transcript_sequence: nt sequence for which ORFs are meant to be identified. Use Plastid method .get_sequence()
    stop_codon_regex: regex (re.compile) to identify stop codons
    
    --Output--
    two lists, the first containing start codons with identified in frame stops, and the second containing those paired stop codons
    
    """
    stops = []
    for x in stop_codon_regex.finditer(transcript_sequence):
        stops.append(x.start())
    
    #Convert to np.array
    starts = np.asarray(start_codon_array)
    stops = np.asarray(stops)
    
    filtered_starts=[]
    filtered_stops=[]
    for identified_start in starts:
        codon_ct = (stops - identified_start)/3 
        codon_ct = codon_ct[codon_ct>0]
        orfs = [a for a in codon_ct if a.is_integer()] #gives a list but thats fine
        
        if len(orfs)>0:
            final_orf_codons = min(orfs)
            predicted_stop = identified_start + final_orf_codons.astype(int)*3 + 3
            filtered_starts.append(identified_start)
            filtered_stops.append(predicted_stop)
        #
    #
    return filtered_starts, filtered_stops

def assign_uORFs_from_harr_peaks(input_transcript, path_to_harringtonine_reads, mapping_for_harringtonine_reads, trained_SVM, scale_logical, scaler_function, genome_twobit, canonical_start_codon, nearcanonical_start_codon, stop_codon, reads_cutoff):
    """Wrapper function for uORF assigment from harringtonine peak data. Workflow as follows:
        (1) Calls tile_SVM_utr5 to create Ingolia vectors for all positions within the 5'UTR of the input transcript and classify using trained SVM. Aggregates positive nt locations into peaks. 
        (2) Loops through peaks and calls find_start_in_peak to annotate canonical or non-canonical start codons
        (3) Loops through UNIQUE start codons identified in peaks and pairs with stop codon by calling pair_start_with_stop
        (4) Loops through start/stop uORF pairs and extracts uORFs as plastid segmentchains, so long as the stop codon is not equal to the cds_stop for that transcript (avoids calling CDS and N-terminal extensions of CDS as uORFs)
        
    --Input--
    input_transcript: plastid transcript object
    path_to_harringtonine_reads: path to bam file for harringtonine reads. Used to make BAMGenomeArray containing Harringtonine read aligments
    mapping_for_harringtonine_reads: path to psite offset file generated by psite script, applies mapping to BAMGenomeArray. 
    trained_SVM: SVM trained to call start_codons based on Ingolia vector
    scale_logical: passed to tile_SVM_utr5. Was the SVM trained on a scaled training set? If true, you must provide a scaler function to transform the generated arrays prior to classification
    scaler_function: passed to tile_SVM_utr5. Function of type preprocessing.StandardScaler().fit(training_array) for given training_array.
    genome_twobit: twobit genome file
    canonical_start_codon: regular expression (re.compile) to find start codons, as required by find_start_in_peak
    nearcanonical_start_codon: regular expression (re.compile) to find near-canonical starts, which will be searched for in peak if a canonical start cannot be found using find_start_in_peak
    stop_codon: regex (re.compile) for stop codons, passed to pair_start_with_stop internally
    reads_cutoff: passed to find_start_in_peak. Used for initial QC of discovered peaks, requring that a peak has > this number of reads in the vector range (-7 to +40 relative to peak bounds)
    
    --Output--
    A list with found uORFs as plastid segmentchains, named as input_transcript.get_name() + start codon location. If no uORFs are found, an empty list.
    """
    print 'Working on ' + input_transcript.get_name() + ' ...'
    
    #Generate Harringtonine Read BAMGenomeArray Interally, as Object cannot otherwise be pickled.
    harringtonine_reads = BAMGenomeArray(path_to_harringtonine_reads)
    harringtonine_reads.set_mapping(VariableFivePrimeMapFactory.from_file(open(mapping_for_harringtonine_reads)))
    
    #Generate the count vector object used to construct Ingolia vectors
    count_vector=input_transcript.get_counts(harringtonine_reads)
    
    #Tile across all 5'UTR and construct vectors at each position, testing with SVM and storing only if predicted start
    predicted_starts=tile_SVM_utr5(input_transcript,count_vector,trained_SVM, scale_logical, scaler_function)
    
    print 'Found ' + str(len(predicted_starts)) + ' positive positions'
    if len(predicted_starts)==0:
        print '...Done!'
        return []
    
    #Concatenate positive positions into peaks 
    positive_ranges=[list(group) for group in mit.consecutive_groups(predicted_starts)]
    print 'Concatenated positions to ' + str(len(positive_ranges)) + ' peak(s)'
    
    #Get Sequence of transcript
    transcript_sequence=input_transcript.get_sequence(genome_twobit)
    
    #Loop through peaks and find start codons
    called_start = []
    for peak in positive_ranges:
        identified_start = find_start_in_peak(peak_range=peak, transcript_sequence=transcript_sequence, count_vector=count_vector, reads_cutoff=reads_cutoff, canonical_start_codon=canonical_start_codon, nearcanonical_start_codon=nearcanonical_start_codon) 
        
        #Append Found Starts
        if type(identified_start) is not str:
            called_start.append(identified_start)
    
    #Find only unique start codons
    called_start=np.asarray(called_start)
    called_start=np.unique(called_start)
    
    if len(called_start)==0:
        print 'Identified 0 start codons.'
        print '...Done!'
        return []
    
    print 'Identified ' + str(len(called_start)) + ' unique start codon(s)'
    
    #Identify uORFs
    orf_start, orf_stop = pair_start_with_stop(start_codon_array=called_start,transcript_sequence=transcript_sequence,stop_codon_regex=stop_codon)
    
    if len(orf_start) == 0 or len(orf_stop)==0:
        print 'Paired 0 start codons.'
        print '...Done!'
        return []
    
    print 'Identified ' + str(len(orf_start)) + ' uORF(s)'
    
    #Probably do not need to look for unique stops because we are predicting peaks based on real data, not computationally where you worry about calling in-frame methionines as new start codons. 
    
    #Extract uORF Sequences as segment chains
    count=0
    segmentchain_list = []
    for start,stop in itertools.izip(orf_start,orf_stop):
        if stop != input_transcript.cds_end:
            discovered_subchain = input_transcript.get_subchain(start,stop) #Cannot simply do my_transcript.get_subchain(start,stop,'ID'='x') b/c hardcoded that ID of subchain = ID transcript + 'subchain'. This apparently cannot be overwritten
            discovered_subchain.attr['ID'] = input_transcript.get_name() + '_' + str(start) #overwrite the attribute to identify uORF uniquely. START this time since we are identifying based on unique start peak and allowing multiple uORF to share the same stop.
            segmentchain_list.append(discovered_subchain)
            count=count+1
        #
    #
    print str(count) + ' uORF(s) passed QC and were appended to list'
    print '...Done!'
    return segmentchain_list

################################################################################
#####        Functions to count reads over various meta-regions            #####
################################################################################

def get_counts_and_lengths_masked(input_segment_chain, mapped_read_array, masked_logical, keep_true):
    """A basic method for getting counts and lengths from a plastid segmentchain and a plastid BAMGenome Array. Advantage over get_counts is that it provides methods for handling masked regions and functionality for keeping either masked OR unmasked region read/length counts.
    
    --Input--
    input_segment_chain: plastid segment_chain_object to calculate parameters over
    mapped_read_array: plastid BAMGenomeArray with mapping set, containing reads that need to be counted
    masked_logical: (True|False) do you want to look at masks that exist on the input segment_chain? Masks must have been previously added with .add_masks method
    keep_true: (string: 'yes'|'no') applicable only when masked_logical=True. Do you want to count reads/length for masked region (yes) or the NON-masked region (no)
    
    --Output--
    a tuple: (counts, length) for input_segment_chain given mapped_read_array
    """
    if masked_logical:
        masked_counts = input_segment_chain.get_masked_counts(mapped_read_array)
        if keep_true == 'yes':
            roi_masked_counts = input_segment_chain.get_counts(mapped_read_array)[masked_counts.mask] #keep only reads in masked region
            my_counts = np.nansum(roi_masked_counts)
            my_length = len(roi_masked_counts)
        #
        else:
            roi_masked_counts = input_segment_chain.get_counts(mapped_read_array)[np.invert(masked_counts.mask)] #Trick b/c get_masked_counts doesnt count the masked regions
            my_counts = np.nansum(roi_masked_counts)
            my_length = len(roi_masked_counts)
        #
    #
    else:
        my_counts = np.nansum(input_segment_chain.get_counts(mapped_read_array))
        my_length = input_segment_chain.get_length()
    #
    return my_counts, my_length

def count_all_meta_regions_from_genewise_dicts(gene_id, full_transcript_dict, meta_uORFs_computational_dict, meta_uORFs_experimental_dict, path_to_bam, path_to_psite):
    """An extremely specialized function to return a list of pertinant values relevant to gene-wise ribosome profiling
    
    --Input--
    gene_id: gene_id for gene of interest. Should be a key in all three dictionaries
    full_transcript_dict: dictionary {gene_id, list of plastid transcript objects. Transcripts should be entire annotation}
    meta_uORFs_computational_dict: dictionary {gene_id, SINGLE plastid transcript object with meta_roi for computationally predicted uORFs for that gene}
    meta_uORFs_experimental_dict: dictionary {gene_id, SINGLE plastid transcript object with meta_roi for experimentally predicted uORFs}
    path_to_bam: as described. passed to function as required for multithreading
    path_to_psite: table generated by plastid psite script giving offsets. see above note.
    
    --Ouput--
    A vector of length 24: [gene_id, gene_name, utr5_counts, utr5_len, utr5_counts_maskedbyCDS, utr5_len_maskedbyCDS, 
    cds_counts, cds_len, utr3_counts, utr3_len, utr3_counts_maskedbyCDS, utr3_len_maskedbyCDS, 
    meta_uORF_comp_counts, meta_uORF_comp_len, meta_uORF_comp_counts_in5utronly, meta_uORF_comp_len_in5utronly, meta_uORF_comp_counts_in5utronly_noCDS, meta_uORF_comp_len_in5utronly_noCDS,
    meta_uORF_exp_counts, meta_uORF_exp_len, meta_uORF_exp_counts_in5utronly, meta_uORF_exp_len_in5utronly, meta_uORF_exp_counts_in5utronly_noCDS, meta_uORF_exp_len_in5utronly_noCDS]
    
    utr_'maskedbyCDS': only portion of meta-utr window that does NOT overlap CDS is counted
    meta_uORF...'in5utronly': only portion of meta-uORF window in the meta-5utr is counted.
    meta_uORF...'in5utronly_noCDS': only portion of meta-uORF window in meta-5utr and NOT in meta-CDS is counted. 
    """
    
    print 'Working on ' + str(gene_id)
    
    #Import Reads and Set mapping
    CHX_reads = BAMGenomeArray(path_to_bam)
    CHX_reads.set_mapping(VariableFivePrimeMapFactory.from_file(open(path_to_psite))) #doesn't read string like it should, so my workaround is to give it the open file handle instead
    
    #Set up meta-window annotations for given gene using full transcript annotations
    transcript_list = full_transcript_dict[gene_id]
    CDS_list = [i.get_cds() for i in transcript_list]
    utr5_list = [i.get_utr5() for i in transcript_list]
    utr3_list = [i.get_utr3() for i in transcript_list]
    meta_CDS = meta_roi(CDS_list)
    meta_utr5 = meta_roi(utr5_list)
    meta_utr3 = meta_roi(utr3_list)
    
    #Mask CDS overlap of 5'UTR and 3'UTR windows. For calculating 5'UTR/CDS/3'UTR ratios most efficiently
    meta_utr5.add_masks(*meta_CDS.segments)
    meta_utr3.add_masks(*meta_CDS.segments)
    
    #Calculate Everything Except for uORF Parameters
    utr5_counts, utr5_len = get_counts_and_lengths_masked(input_segment_chain=meta_utr5, mapped_read_array=CHX_reads, masked_logical=False, keep_true='no')
    utr5_counts_maskedbyCDS, utr5_len_maskedbyCDS = get_counts_and_lengths_masked(input_segment_chain=meta_utr5, mapped_read_array=CHX_reads, masked_logical=True, keep_true='no')
    cds_counts, cds_len = get_counts_and_lengths_masked(input_segment_chain=meta_CDS, mapped_read_array=CHX_reads, masked_logical=False, keep_true='no')
    utr3_counts, utr3_len = get_counts_and_lengths_masked(input_segment_chain=meta_utr3, mapped_read_array=CHX_reads, masked_logical=False, keep_true='no')
    utr3_counts_maskedbyCDS, utr3_len_maskedbyCDS = get_counts_and_lengths_masked(input_segment_chain=meta_utr3, mapped_read_array=CHX_reads, masked_logical=True, keep_true='no')
    
    #Build in if statements b/c may not have uORFs 
    try: 
        meta_uORF_comp = meta_uORFs_computational_dict[gene_id]
        meta_uORF_comp.add_masks(*meta_utr5.segments) 
        meta_uORF_comp_counts, meta_uORF_comp_len = get_counts_and_lengths_masked(input_segment_chain=meta_uORF_comp, mapped_read_array=CHX_reads, masked_logical=False, keep_true='no')
        meta_uORF_comp_counts_in5utronly, meta_uORF_comp_len_in5utronly = get_counts_and_lengths_masked(input_segment_chain=meta_uORF_comp, mapped_read_array=CHX_reads, masked_logical=True, keep_true='yes')
        meta_uORF_comp_in5utr = meta_uORF_comp.get_masks_as_segmentchain()
        meta_uORF_comp_in5utr.add_masks(*meta_CDS.segments)
        meta_uORF_comp_counts_in5utronly_noCDS, meta_uORF_comp_len_in5utronly_noCDS = get_counts_and_lengths_masked(input_segment_chain=meta_uORF_comp_in5utr, mapped_read_array=CHX_reads, masked_logical=True, keep_true='no')
    except KeyError:
        meta_uORF_comp_counts, meta_uORF_comp_len = (np.nan, np.nan)
        meta_uORF_comp_counts_in5utronly, meta_uORF_comp_len_in5utronly = (np.nan, np.nan)
        meta_uORF_comp_counts_in5utronly_noCDS, meta_uORF_comp_len_in5utronly_noCDS = (np.nan, np.nan)
    
    try: 
        meta_uORF_exp = meta_uORFs_experimental_dict[gene_id]
        meta_uORF_exp.add_masks(*meta_utr5.segments) 
        meta_uORF_exp_counts, meta_uORF_exp_len = get_counts_and_lengths_masked(input_segment_chain=meta_uORF_exp, mapped_read_array=CHX_reads, masked_logical=False, keep_true='no')
        meta_uORF_exp_counts_in5utronly, meta_uORF_exp_len_in5utronly = get_counts_and_lengths_masked(input_segment_chain=meta_uORF_exp, mapped_read_array=CHX_reads, masked_logical=True, keep_true='yes')
        meta_uORF_exp_in5utr = meta_uORF_exp.get_masks_as_segmentchain()
        meta_uORF_exp_in5utr.add_masks(*meta_CDS.segments)
        meta_uORF_exp_counts_in5utronly_noCDS, meta_uORF_exp_len_in5utronly_noCDS = get_counts_and_lengths_masked(input_segment_chain=meta_uORF_exp_in5utr, mapped_read_array=CHX_reads, masked_logical=True, keep_true='no')
    except KeyError:
        meta_uORF_exp_counts, meta_uORF_exp_len = (np.nan, np.nan)
        meta_uORF_exp_counts_in5utronly, meta_uORF_exp_len_in5utronly = (np.nan, np.nan)
        meta_uORF_exp_counts_in5utronly_noCDS, meta_uORF_exp_len_in5utronly_noCDS = (np.nan, np.nan)
    
    test = [gene_id, transcript_list[0].attr['gene_id'], utr5_counts, utr5_len, utr5_counts_maskedbyCDS, utr5_len_maskedbyCDS, 
    cds_counts, cds_len, utr3_counts, utr3_len, utr3_counts_maskedbyCDS, utr3_len_maskedbyCDS, 
    meta_uORF_comp_counts, meta_uORF_comp_len, meta_uORF_comp_counts_in5utronly, meta_uORF_comp_len_in5utronly, meta_uORF_comp_counts_in5utronly_noCDS, meta_uORF_comp_len_in5utronly_noCDS,
    meta_uORF_exp_counts, meta_uORF_exp_len, meta_uORF_exp_counts_in5utronly, meta_uORF_exp_len_in5utronly, meta_uORF_exp_counts_in5utronly_noCDS, meta_uORF_exp_len_in5utronly_noCDS]
    
    print '...Done!'
    return test

#Meta-roi is another dependency I wrote very early in working with this type of data. 
#Meta-ROI Function
#Input: built to take output of the anotate_uORF function (a string describing why no uORFs could be found in a transcript OR a list of found uORFs for a transcript as segmentchains)
#Output: If no uORFs in transcript: a string explaining why no uORFs could be found (passed from input). If 1 or more uORFs, a meta-region is constructed which spans all identified uORFs. 
def meta_roi(segmentchain_list):
    if type(segmentchain_list) is str:
        return(segmentchain_list) 
    
    elif (len(segmentchain_list) == 1):
        uORF_meta = segmentchain_list[0]
        return uORF_meta
    
    else:
        # Figure out how to chain the genomic segments together
        segments_list = []
        for i in segmentchain_list[1:len(segmentchain_list)]:
            segments_list.append(i.segments)
        
        segments_flattened = list(itertools.chain.from_iterable(segments_list))
        uORF_meta = segmentchain_list[0] #CAREFUL. This copies the REFERENCE. Thus the next line of code will actually CHANGE founduORFs[0] along with ORF_meta. This is fine for my code here, but just be wary https://stackoverflow.com/questions/19951816/python-changes-to-my-copy-variable-affect-the-original-variable)
        uORF_meta.add_segments(*segments_flattened) 
        return uORF_meta
    #

#######################################################################################
## New Function to Cluster 'Gene Families' Since UCSC doesn't give gene_id in GTF... ##
#######################################################################################
def cluster_segmentchain_families(transcripts_to_annotate):
    """A clustering function. Taking a list of input plastid transcripts, uses a GenomeHash to efficiently partion transcripts into the fewest sets, called 'families', of overlapping transcripts. This is done in a strand-aware way. Useful to group overlapping genes 
    
    --Input--
    transcripts_to_annotate: list of plastid transcript objects to be grouped into families
    
    --Output--
    A dictionary of the form {key:value}. Here key is a string, 'family_chr_start_end_strand' based on maximum spanning genomic segment for the family. Value is a list of plastid transcript objects within that family. 
    """
    annotation_dictionary = {i.get_name():i for i in transcripts_to_annotate}
    hash = GenomeHash(transcripts_to_annotate) #block off spanning segments in genome for each known transcript
    families = defaultdict(list)
    assigned_to_family = []
    for my_transcript in transcripts_to_annotate:
        print 'Working on transcript ' + my_transcript.get_name()
        
        #If transcript has already been included in a family we can safely ignore it b/c family guarenteed to be bounded
        if my_transcript.get_name() in assigned_to_family:
            continue
        
        else:
            #check if spanning segment for my_transcript overlaps anything in the hash
            maximum_segment_family = my_transcript.spanning_segment
            continue_to_search = True
            while continue_to_search:
                #Find overlapping features in genome
                my_family = hash.get_overlapping_features(maximum_segment_family, stranded=True)
                
                #We are searching for any features bigger than self to know if we should expand the family. 
                larger_features = [i for i in my_family if len(i.spanning_segment)>len(my_family)]
                
                #If no features larger than maximum_segment_family our search is done. Break the while loop.
                if len(larger_features) > 0:
                    continue_to_search = False
                
                #Otherwise adjust maximum_segment_family to reflect the largest spanning segment and continue search
                else: 
                    maximum_segment_family = max(larger_features, key=lambda item: len(item.spanning_segment))
                #
            #
            #At this point my_family contains all the transcripts in the family! Output to dictionary named on overlap 
            family_name = 'family_' + maximum_segment_family.chrom + '_' + str(maximum_segment_family.start) + '_' + str(maximum_segment_family.end) + '_' + maximum_segment_family.strand
            families[family_name] = my_family
            #
            #Reserve names of family to assigned_to_family list to narrow search space
            assigned_to_family.extend([i.get_name() for i in my_family])
        #
    #
    return families

#################################################
####       Script for the SVM Approach       ####
#################################################
#Read in Transcript models and Count Data
transcript_list = dill.load(open('path.to.transcripts'))
mm9_genome = twobit.TwoBitFile('path.to.twobit')
CHX_reads = BAMGenomeArray('path.to.bam')
CHX_reads.set_mapping(VariableFivePrimeMapFactory.from_file(open('path.to.psite')))
HRT_reads = BAMGenomeArray('path.to.bam')
HRT_reads.set_mapping(VariableFivePrimeMapFactory.from_file(open('path.to.psite')))

#Read in Ingolia Summaries
Ingolia_supp = pd.read_csv('Supp3.csv') 
Ingolia_well_translated = pd.read_csv('ingolia_supp2a.csv')

#consider only highly translated transcripts to train the SVM. Faster with dict
transcript_dict = defaultdict()
for i in transcript_list:
    transcript_dict[i.get_name()] = i

highly_translated = [transcript_dict[i] for i in Ingolia_well_translated['UCSC ID']]
highly_translated = [i for i in highly_translated if i.get_cds().get_length() > 0]

#Generate arrays on WHOLE dataset (GridSearchCV will do the splitting for you during cross-validation)
codonvectors_grid, assignments_grid = generate_SVM_arrays_around_cds_start(list_input_transcripts=highly_translated, path_to_harringtonine_reads='path.to.bam', path_to_harringtonine_psite='path.to.psite', nthreads=23)

#Scale the Input Arrays and retain the scaler
scaler_grid = preprocessing.StandardScaler().fit(codonvectors_grid)
codonvectors_grid_scale = scaler_grid.transform(codonvectors_grid)

#Cross Validation Approach to Train the SVM
parameters = {'C':[0.1,1,2,4,10],'gamma':[0.001,0.01,0.1,1,2,2.4,3,5, 10], 'class_weight':[{0:1,1:10}, {0:1,1:5}, {0:1,1:4}, {0:1,1:3}, {0:1,1:2}, {0:1,1:1}]}
clf_test = SVC(kernel='rbf', cache_size=1000) 
cv_SVM = GridSearchCV(clf_test, parameters, n_jobs=20, refit=True) #note that we run this over the whole genome
cv_SVM.fit(codonvectors_grid_scale, assignments_grid)
cv_SVM.best_params_ #Wants C=10, gama 0.1, weight 0:1, 1:2
cv_SVM.best_score_
#Find that with setting refit=True for GridSearchCV, can just use test object here to predict (can call test.predict() so it should work)

#Compile Regex for Ranges
canonical_start_codon = re.compile('ATG', re.IGNORECASE)
nearcanonical_start_codon = re.compile('(CTG)|(GTG)|(TTG)|(ACG)|(AGG)|(ATC)|(ATT)|(AAG)|(ATA)', re.IGNORECASE) #figure out non-canonicals.
stop_codon = re.compile('(TAA)|(TAG)|(TGA)', re.IGNORECASE)

#For now lets just look at the transcripts that were in the Ingolia Supp3
transcripts_to_annotate = [transcript_dict[i] for i in Ingolia_supp.knownGene.unique()]
transcripts_to_annotate = [i for i in transcripts_to_annotate if np.isin(i.chrom, mm9_genome.keys())] #7 transcripts not in genome, cannot get uORFs and will throw errors 

#Run Main Script. Note that because we have used refit=True with GridSearchCV we can use the SVM directly in the function (which just requires that the classifier has a .predict() method.)
annotated_ORFs = Pool(23).map(assign_uORFs_from_harr_peaks, transcripts_to_annotate, itertools.repeat('path.to.bam'), itertools.repeat('path.to.psite'), itertools.repeat(cv_SVM), itertools.repeat(True), itertools.repeat(scaler_grid), itertools.repeat(mm9_genome), itertools.repeat(canonical_start_codon), itertools.repeat(nearcanonical_start_codon), itertools.repeat(stop_codon), itertools.repeat(75))

#Reformat uORF annotations in a logical way. Start here if coming back from running a model with known settings. 
annotated_ORFs_final = [i for i in annotated_ORFs if len(i)>0]
annotated_ORFs_final = list(itertools.chain.from_iterable(annotated_ORFs_final))

#################################################
##     Script to Compare my uORFs to Ingolia   ##
#################################################

Ingolia_uORFs_table = Ingolia_supp[np.isin(Ingolia_supp.Product, ['uorf', 'uorf-overlap'])]

peaks_table = pd.DataFrame(index=range(0,len(annotated_ORFs_final)), columns=['knownGene', 'Init Codon [nt]'])
extract_start = re.compile(".*?_([0-9]+)")
for index, transcript in enumerate(annotated_ORFs_final):
    peaks_table['knownGene'][index] = transcript.attr['transcript_id']
    regex_match = extract_start.match(transcript.get_name())
    peaks_table['Init Codon [nt]'][index] = int(regex_match.groups()[0])

peaks_table['Init Codon [nt]'] = peaks_table['Init Codon [nt]'].apply(int)
overlap = peaks_table.merge(Ingolia_uORFs_table, how = 'inner', on=['knownGene','Init Codon [nt]'])

#Performance is good but a little different. 
overlap.shape[0]/Ingolia_uORFs_table.shape[0] #58% of Ingolia uORFs identified with class_weight 4 as in Ingolia
(peaks_table.shape[0]-overlap.shape[0])/peaks_table.shape[0] #33% of our calls were not in Ingolia class_weight 4 as in Ingolia

#A couple other ways to look for differences...
peaks_myscript = defaultdict(list)
for x in peaks_table.itertuples():
    index, transcript, start = x
    peaks_myscript[transcript].append(start)

peaks_Ingolia = defaultdict(list)
for x in Ingolia_uORFs_table.itertuples():
    peaks_Ingolia[x[1]].append(x[3])

for i in peaks_myscript.keys()[0:10]:
    print i
    print peaks_myscript[i]
    print peaks_Ingolia[i]

#Show that searching over Ingolia Supp3 transcripts is reasonable. Indeed basically everything in their highly_translated 
highly_translated_names = [i.get_name() for i in highly_translated]
test = np.isin(Ingolia_supp.knownGene.unique(), highly_translated_names)
sum(test) 
len(Ingolia_supp.knownGene.unique())
#They are basically the same thing. We are fine. 

#Check if we have any duplicates in my list (ie. do we need to adjust names)
annotated_ORFs_final_names = [i.get_name() for i in annotated_ORFs_final]
len(np.unique(annotated_ORFs_final_names)) == len(annotated_ORFs_final_names)

#Save my uORF list.
dill.dump(annotated_ORFs_final, open('uORFs_ingoliaset_mycrossvalidation.obj','wb'))

#################################################
##  Test performance of my vs. Ingolia uORFs   ##
#################################################

ingolia_uORFs = dill.load(open('./updated_quantitation/ingolia_uORFs.obj'))
my_uORFs = deepcopy(annotated_ORFs_final) #for readability 
#Use 'transcripts_to_annotate', as this is the set of transcripts from Ingolia Known_Gene, and was the starting point for my annotations. Use those as the 'genome' for our test.

#Empirically find transcripts that overlap and collapse with a common gene_id. These ensures that we do not count reads 2x. 
clustered_transcripts = cluster_segmentchain_families(transcripts_to_annotate)
my_dict = {key: value for (key,value) in clustered_transcripts.iteritems() if len(value)>1} #identify interesting sets to check they clustered correctly

#Check shows that this looks good in the genome browser on this dataset. Will need further testing if used for a more complex clustering approach. 
fout = open('transcripts_to_annotate.bed','w')
for i in transcripts_to_annotate:
    fout.write(i.as_bed())

fout.close()

#Lookup Dictionary: {transcript_id: family_name}
inv_clustered_transcripts = {}
for gene, transcript_list in clustered_transcripts.iteritems():
    for transcript in transcript_list:
        inv_clustered_transcripts[transcript.get_name()] = gene
    #

#Full Transcript Dictionary
full_transcript_dict = defaultdict(list)
for i in transcripts_to_annotate:
    family_name = inv_clustered_transcripts[i.get_name()]
    full_transcript_dict[family_name].append(i)

#Ingolia meta-uORF Dictionary
ingolia_uORF_dict = defaultdict(list)
for i in ingolia_uORFs:
    family_name = inv_clustered_transcripts[i.attr['transcript_id']]
    ingolia_uORF_dict[family_name].append(i)

ingolia_uORF_dict = {key: meta_roi(value) for (key, value) in ingolia_uORF_dict.iteritems()}

#My meta-uORF dictionary
my_uORF_dict = defaultdict(list)
for i in my_uORFs:
    family_name = inv_clustered_transcripts[i.attr['transcript_id']]
    my_uORF_dict[family_name].append(i)

my_uORF_dict = {key: meta_roi(value) for (key, value) in my_uORF_dict.iteritems()}

#Run the Counter Script across gene_families
output_array= []
for i in full_transcript_dict.iterkeys():
    output_array.append(count_all_meta_regions_from_genewise_dicts(i, full_transcript_dict = full_transcript_dict, meta_uORFs_computational_dict= ingolia_uORF_dict, meta_uORFs_experimental_dict=my_uORF_dict, path_to_bam = 'path.to.bam', path_to_psite= 'path.to.psite'))

#Convert counter output to dataframe 
output_array_test = pd.DataFrame(output_array, columns = ('gene_id','gene_name', 'utr5_counts', 'utr5_len', 'utr5_counts_maskedbyCDS', 'utr5_len_maskedbyCDS', 
    'cds_counts', 'cds_len', 'utr3_counts', 'utr3_len', 'utr3_counts_maskedbyCDS', 'utr3_len_maskedbyCDS', 
    'meta_uORF_ingolia_counts', 'meta_uORF_ingolia_len', 'meta_uORF_ingolia_counts_in5utronly', 'meta_uORF_ingolia_len_in5utronly', 'meta_uORF_ingolia_counts_in5utronly_noCDS', 'meta_uORF_ingolia_len_in5utronly_noCDS',
    'meta_uORF_myscript_counts', 'meta_uORF_myscript_len', 'meta_uORF_myscript_counts_in5utronly', 'meta_uORF_myscript_len_in5utronly', 'meta_uORF_myscript_counts_in5utronly_noCDS', 'meta_uORF_myscript_len_in5utronly_noCDS'))

#Set columns 2:end to type int
cols = output_array_test.columns.tolist()
cols = cols[2:] 
output_array_test[cols] = output_array_test[cols].astype(np.float)
output_array_test.to_csv(path_or_buf='counts_ingoliauORFs_vs_myuORFs.tsv',sep='\t',index=False,header=True)

total_5utr_counts = np.nansum(output_array_test.utr5_counts_maskedbyCDS)
total_5utr_len = np.nansum(output_array_test.utr5_len_maskedbyCDS)

ingolia_5utr_counts = np.nansum(output_array_test.meta_uORF_ingolia_counts_in5utronly_noCDS)
ingolia_5utr_len = np.nansum(output_array_test.meta_uORF_ingolia_len_in5utronly_noCDS)

me_5utr_counts = np.nansum(output_array_test.meta_uORF_myscript_counts_in5utronly_noCDS)
me_5utr_len = np.nansum(output_array_test.meta_uORF_myscript_len_in5utronly_noCDS)

#Score ingolia
ingolia_5utr_counts/total_5utr_counts #59.667% of reads
ingolia_5utr_len/total_5utr_len #34% length

#Score mine
me_5utr_counts/total_5utr_counts #62.29% of reads
me_5utr_len/total_5utr_len #28.67% of reads

#Quickly compute 5'UTR, CDS and 3'UTR QC stats for the ES cells in mouse
utr5_rpnt = np.nansum(output_array_test.utr5_counts_maskedbyCDS) / np.nansum(output_array_test.utr5_len_maskedbyCDS) * 1000
cds_rpnt = np.nansum(output_array_test.cds_counts) / np.nansum(output_array_test.cds_len) * 1000
utr3_rpnt = np.nansum(output_array_test.utr3_counts_maskedbyCDS) / np.nansum(output_array_test.utr3_len_maskedbyCDS) * 1000

#QC Stats for utr/cds bias
cds_rpnt / utr5_rpnt #1.29 
cds_rpnt / utr3_rpnt #152.5