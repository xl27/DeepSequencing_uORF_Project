##################################################
#####        Import Relevant Packages        #####
##################################################

#Basic ribosome profiling packages
from __future__ import division
from plastid import GTF2_TranscriptAssembler, BAMGenomeArray, VariableFivePrimeMapFactory, GenomeHash
import twobitreader as twobit
import numpy as np
import re
from collections import Counter, defaultdict
import itertools
from plastid.util.io.filters import CommentReader
import pandas as pd

#Specific packages for this script
import more_itertools as mit
import dill
from pathos.multiprocessing import ProcessingPool as Pool
from copy import deepcopy

################################################################################
#####        Functions to count reads over various meta-regions            #####
################################################################################
def get_counts_and_lengths_masked(input_segment_chain, mapped_read_array, masked_logical, keep_true):
    """A basic method for getting counts and lengths from a plastid segmentchain and a plastid BAMGenome Array. Advantage over get_counts is that it provides methods for handling masked regions and functionality for keeping either masked OR unmasked region read/length counts.
    
    --Input--
    input_segment_chain: plastid segment_chain_object to calculate parameters over
    mapped_read_array: plastid BAMGenomeArray with mapping set, containing reads that need to be counted
    masked_logical: (True|False) do you want to look at masks that exist on the input segment_chain? Masks must have been previously added with .add_masks method
    keep_true: (string: 'yes'|'no') applicable only when masked_logical=True. Do you want to count reads/length for masked region (yes) or the NON-masked region (no)
    
    --Output--
    a tuple: (counts, length) for input_segment_chain given mapped_read_array
    """
    if masked_logical:
        masked_counts = input_segment_chain.get_masked_counts(mapped_read_array)
        if keep_true == 'yes':
            roi_masked_counts = input_segment_chain.get_counts(mapped_read_array)[masked_counts.mask] #keep only reads in masked region
            my_counts = np.nansum(roi_masked_counts)
            my_length = len(roi_masked_counts)
        #
        else:
            roi_masked_counts = input_segment_chain.get_counts(mapped_read_array)[np.invert(masked_counts.mask)] #Trick b/c get_masked_counts doesnt count the masked regions
            my_counts = np.nansum(roi_masked_counts)
            my_length = len(roi_masked_counts)
        #
    #
    else:
        my_counts = np.nansum(input_segment_chain.get_counts(mapped_read_array))
        my_length = input_segment_chain.get_length()
    #
    return my_counts, my_length

def count_all_meta_regions_from_genewise_dicts(gene_id, full_transcript_dict, meta_uORFs_computational_dict, meta_uORFs_experimental_dict, path_to_bam, path_to_psite):
    """An extremely specialized function to return a list of pertinant values relevant to gene-wise ribosome profiling
    
    --Input--
    gene_id: gene_id for gene of interest. Should be a key in all three dictionaries
    full_transcript_dict: dictionary {gene_id, list of plastid transcript objects. Transcripts should be entire annotation}
    meta_uORFs_computational_dict: dictionary {gene_id, SINGLE plastid transcript object with meta_roi for computationally predicted uORFs for that gene}
    meta_uORFs_experimental_dict: dictionary {gene_id, SINGLE plastid transcript object with meta_roi for experimentally predicted uORFs}
    path_to_bam: as described. passed to function as required for multithreading
    path_to_psite: table generated by plastid psite script giving offsets. see above note.
    
    --Ouput--
    A vector of length 24: [gene_id, gene_name, utr5_counts, utr5_len, utr5_counts_maskedbyCDS, utr5_len_maskedbyCDS, 
    cds_counts, cds_len, utr3_counts, utr3_len, utr3_counts_maskedbyCDS, utr3_len_maskedbyCDS, 
    meta_uORF_comp_counts, meta_uORF_comp_len, meta_uORF_comp_counts_in5utronly, meta_uORF_comp_len_in5utronly, meta_uORF_comp_counts_in5utronly_noCDS, meta_uORF_comp_len_in5utronly_noCDS,
    meta_uORF_exp_counts, meta_uORF_exp_len, meta_uORF_exp_counts_in5utronly, meta_uORF_exp_len_in5utronly, meta_uORF_exp_counts_in5utronly_noCDS, meta_uORF_exp_len_in5utronly_noCDS]
    
    utr_'maskedbyCDS': only portion of meta-utr window that does NOT overlap CDS is counted
    meta_uORF...'in5utronly': only portion of meta-uORF window in the meta-5utr is counted.
    meta_uORF...'in5utronly_noCDS': only portion of meta-uORF window in meta-5utr and NOT in meta-CDS is counted. 
    """
    
    print 'Working on ' + str(gene_id)
    
    #Import Reads and Set mapping
    CHX_reads = BAMGenomeArray(path_to_bam)
    CHX_reads.set_mapping(VariableFivePrimeMapFactory.from_file(open(path_to_psite))) #doesn't read string like it should, so my workaround is to give it the open file handle instead
    
    #Set up meta-window annotations for given gene using full transcript annotations
    transcript_list = full_transcript_dict[gene_id]
    CDS_list = [i.get_cds() for i in transcript_list]
    utr5_list = [i.get_utr5() for i in transcript_list]
    utr3_list = [i.get_utr3() for i in transcript_list]
    meta_CDS = meta_roi(CDS_list)
    meta_utr5 = meta_roi(utr5_list)
    meta_utr3 = meta_roi(utr3_list)
    
    #Mask CDS overlap of 5'UTR and 3'UTR windows. For calculating 5'UTR/CDS/3'UTR ratios most efficiently
    meta_utr5.add_masks(*meta_CDS.segments)
    meta_utr3.add_masks(*meta_CDS.segments)
    
    #Calculate Everything Except for uORF Parameters
    utr5_counts, utr5_len = get_counts_and_lengths_masked(input_segment_chain=meta_utr5, mapped_read_array=CHX_reads, masked_logical=False, keep_true='no')
    utr5_counts_maskedbyCDS, utr5_len_maskedbyCDS = get_counts_and_lengths_masked(input_segment_chain=meta_utr5, mapped_read_array=CHX_reads, masked_logical=True, keep_true='no')
    cds_counts, cds_len = get_counts_and_lengths_masked(input_segment_chain=meta_CDS, mapped_read_array=CHX_reads, masked_logical=False, keep_true='no')
    utr3_counts, utr3_len = get_counts_and_lengths_masked(input_segment_chain=meta_utr3, mapped_read_array=CHX_reads, masked_logical=False, keep_true='no')
    utr3_counts_maskedbyCDS, utr3_len_maskedbyCDS = get_counts_and_lengths_masked(input_segment_chain=meta_utr3, mapped_read_array=CHX_reads, masked_logical=True, keep_true='no')
    
    #Build in if statements b/c may not have uORFs 
    try: 
        meta_uORF_comp = meta_uORFs_computational_dict[gene_id]
        meta_uORF_comp.add_masks(*meta_utr5.segments) 
        meta_uORF_comp_counts, meta_uORF_comp_len = get_counts_and_lengths_masked(input_segment_chain=meta_uORF_comp, mapped_read_array=CHX_reads, masked_logical=False, keep_true='no')
        meta_uORF_comp_counts_in5utronly, meta_uORF_comp_len_in5utronly = get_counts_and_lengths_masked(input_segment_chain=meta_uORF_comp, mapped_read_array=CHX_reads, masked_logical=True, keep_true='yes')
        meta_uORF_comp_in5utr = meta_uORF_comp.get_masks_as_segmentchain()
        meta_uORF_comp_in5utr.add_masks(*meta_CDS.segments)
        meta_uORF_comp_counts_in5utronly_noCDS, meta_uORF_comp_len_in5utronly_noCDS = get_counts_and_lengths_masked(input_segment_chain=meta_uORF_comp_in5utr, mapped_read_array=CHX_reads, masked_logical=True, keep_true='no')
    except KeyError:
        meta_uORF_comp_counts, meta_uORF_comp_len = (np.nan, np.nan)
        meta_uORF_comp_counts_in5utronly, meta_uORF_comp_len_in5utronly = (np.nan, np.nan)
        meta_uORF_comp_counts_in5utronly_noCDS, meta_uORF_comp_len_in5utronly_noCDS = (np.nan, np.nan)
    
    try: 
        meta_uORF_exp = meta_uORFs_experimental_dict[gene_id]
        meta_uORF_exp.add_masks(*meta_utr5.segments) 
        meta_uORF_exp_counts, meta_uORF_exp_len = get_counts_and_lengths_masked(input_segment_chain=meta_uORF_exp, mapped_read_array=CHX_reads, masked_logical=False, keep_true='no')
        meta_uORF_exp_counts_in5utronly, meta_uORF_exp_len_in5utronly = get_counts_and_lengths_masked(input_segment_chain=meta_uORF_exp, mapped_read_array=CHX_reads, masked_logical=True, keep_true='yes')
        meta_uORF_exp_in5utr = meta_uORF_exp.get_masks_as_segmentchain()
        meta_uORF_exp_in5utr.add_masks(*meta_CDS.segments)
        meta_uORF_exp_counts_in5utronly_noCDS, meta_uORF_exp_len_in5utronly_noCDS = get_counts_and_lengths_masked(input_segment_chain=meta_uORF_exp_in5utr, mapped_read_array=CHX_reads, masked_logical=True, keep_true='no')
    except KeyError:
        meta_uORF_exp_counts, meta_uORF_exp_len = (np.nan, np.nan)
        meta_uORF_exp_counts_in5utronly, meta_uORF_exp_len_in5utronly = (np.nan, np.nan)
        meta_uORF_exp_counts_in5utronly_noCDS, meta_uORF_exp_len_in5utronly_noCDS = (np.nan, np.nan)
    
    test = [gene_id, transcript_list[0].attr['gene_id'], utr5_counts, utr5_len, utr5_counts_maskedbyCDS, utr5_len_maskedbyCDS, 
    cds_counts, cds_len, utr3_counts, utr3_len, utr3_counts_maskedbyCDS, utr3_len_maskedbyCDS, 
    meta_uORF_comp_counts, meta_uORF_comp_len, meta_uORF_comp_counts_in5utronly, meta_uORF_comp_len_in5utronly, meta_uORF_comp_counts_in5utronly_noCDS, meta_uORF_comp_len_in5utronly_noCDS,
    meta_uORF_exp_counts, meta_uORF_exp_len, meta_uORF_exp_counts_in5utronly, meta_uORF_exp_len_in5utronly, meta_uORF_exp_counts_in5utronly_noCDS, meta_uORF_exp_len_in5utronly_noCDS]
    
    print '...Done!'
    return test

#Meta-roi is another dependency I wrote very early in working with this type of data. 
#Meta-ROI Function
#Input: built to take output of the anotate_uORF function (a string describing why no uORFs could be found in a transcript OR a list of found uORFs for a transcript as segmentchains)
#Output: If no uORFs in transcript: a string explaining why no uORFs could be found (passed from input). If 1 or more uORFs, a meta-region is constructed which spans all identified uORFs. 
def meta_roi(segmentchain_list):
    if type(segmentchain_list) is str:
        return(segmentchain_list) 
    
    elif (len(segmentchain_list) == 1):
        uORF_meta = segmentchain_list[0]
        return uORF_meta
    
    else:
        # Figure out how to chain the genomic segments together
        segments_list = []
        for i in segmentchain_list[1:len(segmentchain_list)]:
            segments_list.append(i.segments)
        
        segments_flattened = list(itertools.chain.from_iterable(segments_list))
        uORF_meta = segmentchain_list[0] #CAREFUL. This copies the REFERENCE. Thus the next line of code will actually CHANGE founduORFs[0] along with ORF_meta. This is fine for my code here, but just be wary https://stackoverflow.com/questions/19951816/python-changes-to-my-copy-variable-affect-the-original-variable)
        uORF_meta.add_segments(*segments_flattened) 
        return uORF_meta
    #

#######################################################################################
## New Function to Cluster 'Gene Families' Since UCSC doesn't give gene_id in GTF... ##
#######################################################################################
def cluster_segmentchain_families(transcripts_to_annotate):
    """A clustering function. Taking a list of input plastid transcripts, uses a GenomeHash to efficiently partion transcripts into the fewest sets, called 'families', of overlapping transcripts. This is done in a strand-aware way. Useful to group overlapping genes 
    
    --Input--
    transcripts_to_annotate: list of plastid transcript objects to be grouped into families
    
    --Output--
    A dictionary of the form {key:value}. Here key is a string, 'family_chr_start_end_strand' based on maximum spanning genomic segment for the family. Value is a list of plastid transcript objects within that family. 
    """
    annotation_dictionary = {i.get_name():i for i in transcripts_to_annotate}
    hash = GenomeHash(transcripts_to_annotate) #block off spanning segments in genome for each known transcript
    families = defaultdict(list)
    assigned_to_family = []
    for my_transcript in transcripts_to_annotate:
        print 'Working on transcript ' + my_transcript.get_name()
        
        #If transcript has already been included in a family we can safely ignore it b/c family guarenteed to be bounded
        if my_transcript.get_name() in assigned_to_family:
            continue
        
        else:
            #check if spanning segment for my_transcript overlaps anything in the hash
            maximum_segment_family = my_transcript.spanning_segment
            continue_to_search = True
            while continue_to_search:
                #Find overlapping features in genome
                my_family = hash.get_overlapping_features(maximum_segment_family, stranded=True)
                
                #We are searching for any features bigger than self to know if we should expand the family. 
                larger_features = [i for i in my_family if len(i.spanning_segment)>len(my_family)]
                
                #If no features larger than maximum_segment_family our search is done. Break the while loop.
                if len(larger_features) > 0:
                    continue_to_search = False
                
                #Otherwise adjust maximum_segment_family to reflect the largest spanning segment and continue search
                else: 
                    maximum_segment_family = max(larger_features, key=lambda item: len(item.spanning_segment))
                #
            #
            #At this point my_family contains all the transcripts in the family! Output to dictionary named on overlap 
            family_name = 'family_' + maximum_segment_family.chrom + '_' + str(maximum_segment_family.start) + '_' + str(maximum_segment_family.end) + '_' + maximum_segment_family.strand
            families[family_name] = my_family
            #
            #Reserve names of family to assigned_to_family list to narrow search space
            assigned_to_family.extend([i.get_name() for i in my_family])
        #
    #
    return families

########################################
#####       Analysis Script        #####
########################################
#Load up the Annotations and Genome Data
hg38_transcripts = dill.load(open('hg38_plastid_transcripts.obj'))
hg38_genome = twobit.TwoBitFile('path.to.2bit')

#We analyzed just the highly translated set, so let's do the QC on that subset first
highly_translated = dill.load(open("hg38_highlytranslated_transcripts.obj")) #see details of this from the HEK293T_analysis.py script

#Load in our identified uORFs. 
annotated_ORFs_final = dill.load(open('HEK293T_uORFs.obj')) #again, see details fo this from HEK293T_analysis.py script

#Cluster Families within highly_translated so that we avoid overlapping reads. This is a dictionary.
clustered_transcripts = cluster_segmentchain_families(highly_translated)

#Create dictionary to map transcripts back to family
inv_clustered_transcripts = {}
for gene, transcript_list in clustered_transcripts.iteritems():
    for transcript in transcript_list:
        inv_clustered_transcripts[transcript.get_name()] = gene
    #

#Create dictionary of family name: full transcript objects.
full_transcript_dict = defaultdict(list)
for i in highly_translated:
    family_name = inv_clustered_transcripts[i.get_name()]
    full_transcript_dict[family_name].append(i)

#Create dictionary of family names: uORF objects
uORF_dictionary = defaultdict(list)
for i in annotated_ORFs_final:
    family_name = inv_clustered_transcripts[i.attr['transcript_id']]
    uORF_dictionary[family_name].append(i)

uORF_dictionary = {key: meta_roi(value) for (key, value) in uORF_dictionary.iteritems()}

#Run the count_all_meta_regions function. Simply give it the uORF dicitonary twice and cut out the extra one later
output_array= []
for i in full_transcript_dict.iterkeys():
    output_array.append(count_all_meta_regions_from_genewise_dicts(i, full_transcript_dict = full_transcript_dict, meta_uORFs_computational_dict= uORF_dictionary, meta_uORFs_experimental_dict=uORF_dictionary, path_to_bam = 'path.to.bam', path_to_psite= 'path.to.psite'))

output_array_final = pd.DataFrame(output_array, columns = ('gene_id','gene_name', 'utr5_counts', 'utr5_len', 'utr5_counts_maskedbyCDS', 'utr5_len_maskedbyCDS', 
    'cds_counts', 'cds_len', 'utr3_counts', 'utr3_len', 'utr3_counts_maskedbyCDS', 'utr3_len_maskedbyCDS', 'meta_uORF_counts', 'meta_uORF_len', 'meta_uORF_counts_in5utronly', 'meta_uORF_len_in5utronly', 'meta_uORF_counts_in5utronly_noCDS', 'meta_uORF_len_in5utronly_noCDS', 'meta_uORF_myscript_counts', 'meta_uORF_myscript_len', 'meta_uORF_myscript_counts_in5utronly', 'meta_uORF_myscript_len_in5utronly', 'meta_uORF_myscript_counts_in5utronly_noCDS', 'meta_uORF_myscript_len_in5utronly_noCDS'))

output_array_final = output_array_final.iloc[:,0:18]

#Analyze performance of script across the transcripts that were considered.
cols = output_array_final.columns.tolist()
cols = cols[2:] 
output_array_final[cols] = output_array_final[cols].astype(np.float)

#Sum important columns
total_5utr_counts = np.nansum(output_array_final.utr5_counts_maskedbyCDS)
total_5utr_len = np.nansum(output_array_final.utr5_len_maskedbyCDS)

uORF_5utr_counts = np.nansum(output_array_final.meta_uORF_counts_in5utronly_noCDS)
uORF_5utr_len = np.nansum(output_array_final.meta_uORF_len_in5utronly_noCDS)

#See how we did
uORF_5utr_counts/total_5utr_counts #37% 
uORF_5utr_len/total_5utr_len #5%

#Compare 5'UTR RPKM vs. CDS vs. 3'UTR in these datasets
utr5_rpnt = (np.nansum(output_array_final.utr5_counts_maskedbyCDS)/np.nansum(output_array_final.utr5_len_maskedbyCDS)) * 1000
cds_rpnt = (np.nansum(output_array_final.cds_counts)/np.nansum(output_array_final.cds_len)) * 1000
utr3_rpnt = (np.nansum(output_array_final.utr3_counts_maskedbyCDS)/np.nansum(output_array_final.utr3_len_maskedbyCDS)) * 1000

cds_rpnt / utr5_rpnt #8.8x 
cds_rpnt / utr3_rpnt #43.5x
