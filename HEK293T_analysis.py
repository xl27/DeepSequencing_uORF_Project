##################################################
#####            Import Packages             #####
##################################################

from __future__ import division
from plastid import GTF2_TranscriptAssembler, BAMGenomeArray, VariableFivePrimeMapFactory, GenomeHash, CenterMapFactory
import twobitreader as twobit
import numpy as np
import re
from collections import Counter, defaultdict
import itertools
from plastid.util.io.filters import CommentReader

#Specific packages for this script
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import more_itertools as mit
import dill
from pathos.multiprocessing import ProcessingPool as Pool
from copy import deepcopy

#Machine Learning
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

##################################################
#####     Functions to Process CHX Data      #####
##################################################

def count_CHXreads_to_dataframe(list_of_transcripts, path_to_CHX_reads, path_to_CHX_psite, nthreads):
    """A multithreaded function which returns a pandas dataframe containing a plastid transcript object and its CHX read counts, CDS lengths, rpnt for a list of plastid transcript objects. This is helpful for percentile-based subsetting of transcript list for SVM training. 
    
    --Inputs--
    list_of_transcripts: list of plastid transcript objects. All objects should have a CDS annotated, as we will count CHX reads in the CDS to approximate translation
    path_to_CHX_reads: string giving path to .bam file for CHX read data in the tissue of interest
    path_to_CHX_psite: string giving path to file containing psite offsets (generated by plastid psite script) for transcripts of interest. 
    
    --Output--
    Pandas dataframe of dimension (n input transcript, 4) with columns ['segmentchain','cds_reads','length','rpnt'] which are self-explanatory. Segmentchain column contains actual transcript objects to facilitate out-of-the-box application as an iterable for downstream work (ie. no need to look up the transcripts after subsetting).
    """
    def count_CHX_single_transcript(input_transcript, path_to_CHX_reads, path_to_CHX_psite):
        print 'Working On ' + str(input_transcript.get_name()) + '...'
        
        #Prepare reads and transcript object
        CHX_reads = BAMGenomeArray(path_to_CHX_reads)
        CHX_reads.set_mapping(VariableFivePrimeMapFactory.from_file(open(path_to_CHX_psite)))
        cds_transcript=input_transcript.get_cds()
        
        #Count features
        counts=np.nansum(cds_transcript.get_counts(CHX_reads))
        length=cds_transcript.get_length()
        rpnt_cds=counts/length
        print '...Done!'
        return input_transcript, counts, length, rpnt_cds
    
    output = Pool(nthreads).map(count_CHX_single_transcript, list_of_transcripts, itertools.repeat(path_to_CHX_reads), itertools.repeat(path_to_CHX_psite))
    
    return pd.DataFrame(output, columns = ['segmentchain','cds_reads','length','rpnt'])


##################################################
##### Functions to Construct Ingolia Vectors #####
##################################################

def get_codon_count_from_relative_position(start_codon_absolute,rel_codon_pos, count_vector):
    """Get a codon read count (-1, 0, +1), when codon is specified as a relative position from a start codon, whose absolute position in transcript coordinates is known.
    
    Requires location of the start codon (i.cds_start attribute if a known start codon), relative codon position and a count vector for that transcript (i.get_counts(my_BAMgenomearray))
    
    --Input--
    start_codon_absolute: fixed point on transcript (nt, in transcript coordinates) from which relative position will be defined. For example, when constructing the Ingolia vector, this would be the transcript coordinate for the cds_start (ie. i.cds_start attribute of plastid transcript object). 
    rel_codon_position: distance in CODONS from fixed point to codon (-1,0,1) where read count is desired
    count_vector: a vector of counts at each position of transcript being used to construct vectors. Use object.get_counts(BAM array) to generate.
    
    --Output--
    An integer with the read count at the desired codon position, relative to a fixed starting point
     
    """
    
    codon_position_in_nt= start_codon_absolute + (rel_codon_pos * 3)
    final_count=np.nansum(count_vector[codon_position_in_nt-1:codon_position_in_nt+2]) #get codon score -1,-,+1 inclusive.
    return final_count

def construct_Ingolia_vector(site_to_zero, count_vector):
    """Construct an 8 item vector of codon-wise read counts as per Ingolia 2011. 
    Codon positions summed as [[−2, −1], [0], [1], [2], [3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13]] relative to candidate codon at position 0, reads taken at nt -1,0,1 for each codon. 
    
    --Input--
    site_to_zero: position, specified in transcript coordinates, around which the Ingolia vector should be constructed. This function will take that position, use 'get_codon_count_from_relative_position' to count reads at proper codon locations relative to the position, and return the vector
    count_vector: passed to get_codon_count_from_relative_position. See description there.
    
    --Output--
    A vector of length 8 as specified by Ingolia 2011 Cell for given nucleotide position, site_to_zero
    
    """
    codon_vector=[[-2, -1], [0], [1], [2], [3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13]]
    final_vector=[]
    for position in codon_vector:
        position_counts=map(lambda iter: get_codon_count_from_relative_position(site_to_zero,iter,count_vector), position)
        vector_count=sum(position_counts)
        final_vector.append(vector_count)
    
    return final_vector    

##########################################################
##### Functions to Construct Arrays for Training SVM #####
##########################################################

def generate_SVM_arrays_around_cds_start(list_input_transcripts, path_to_harringtonine_reads, path_to_harringtonine_psite, nthreads):
    """Generates positive and negative vectors for training SVM to predict start peaks using Harringtonine ribosome profiling data. 
        (1) Loops through list of input transcripts, generating positive and negative example vectors from each transcript
            (a) Using the annotated_CDS_start for each transcript as a positive example, constructs the Ingolia vector 
            (b) Constructs Ingolia vectors for 10 negative locations on each transcript as specified in 2011 cell paper 
        (2) Concatenates positive and negative vectors into a single numpy array which is output
        (3) Returns a second array with labels for each vector (1=positive example, 0=negative example)
        
        --Input--
        list_input_transcripts: iterable containing plastid transcript objects to be processed. All supplied transcripts are processed, so be sure you have filtered for highly expressed transcripts, and split into test/training groups beforehand.
        path_to_harringtonine_reads: path to .bam file containing harringtonine reads used to construct the arrays
        path_to_harringtonine_psite: path to file generated by plastid psite script giving psite offsets for harringtonine reads.
        nthreads: number cores to use for parallel processing. Please note that this uses pathos.multiprocessing ProcessingPool and it expects to find 'Pool' defined globally. You MUST run 'from pathos.multiprocessing import ProcessingPool as Pool' before trying this function!
        
        --Output--
        Two np arrays: the first contains Ingolia vectors constructed from the input list. The second contains assignments (1=positive example, ie. vector constructed from annotated CDS_start locations. 0=negative example, constructed as specified in 2011 Cell paper). There are 10 negative examples per 1 positive example on each of the input transcripts passing basic QC filtering.
    """
    def process_single_transcript_forSVM(input_transcript, path_to_harringtonine_reads, path_to_harringtonine_psite):
        print 'Working on ' + input_transcript.get_name() + '...'
        #Set up harringtonine reads
        harringtonine_reads = BAMGenomeArray(path_to_harringtonine_reads)
        harringtonine_reads.set_mapping(VariableFivePrimeMapFactory.from_file(open(path_to_harringtonine_psite)))
        
        #Set up vectors to append into
        positive_vectors=[]
        negative_vectors=[]
        
        #Ensure transcript competant to be a test example
        start_codon_nt = input_transcript.cds_start
        if not start_codon_nt-25 > 0 or not start_codon_nt+190 < input_transcript.get_length():#Ingolia says -18 on each side of initiation site scoring window. Noting that scoring window is -7 to 40 nt from given site, accounting for negative vector should be 150+40=190 on positive bound.
            return positive_vectors, negative_vectors
        
        #Create the vectors
        count_vector=input_transcript.get_counts(harringtonine_reads)
        positive_vectors.append(construct_Ingolia_vector(start_codon_nt,count_vector))
        for z in [-6,-3,3,9,18,30,60,90,120,150]:
            negative_vectors.append(construct_Ingolia_vector(start_codon_nt+z,count_vector))
        #
        print '...Done!'
        return positive_vectors, negative_vectors
    #
    output = Pool(nthreads).map(process_single_transcript_forSVM, list_input_transcripts, itertools.repeat(path_to_harringtonine_reads), itertools.repeat(path_to_harringtonine_psite))
    
    #Unpack mapped output. Each iteration is output as a list, which contains two sub-lists with positive and negative vectors
    positive_vectors = [x[0] for x in output if len(x[0])>0]
    negative_vectors = [x[1] for x in output if len(x[0])>0] #if x[0] = [], x[1] will also = []. This just removes iterations where the transcript didn't pass QC
    
    #Second level of unpacking. Take list of lists of vectors to just a list of vectors
    positive_vectors = list(itertools.chain.from_iterable(positive_vectors))
    negative_vectors = list(itertools.chain.from_iterable(negative_vectors))
    
    ingolia_vector_array=np.asarray(positive_vectors+negative_vectors)   
    identities_for_array=np.asarray([1]*len(positive_vectors) + [0]*len(negative_vectors))
    #
    return ingolia_vector_array, identities_for_array

################################################################################
##### Functions using Trained SVM to predict uORFs from Harringtonine Data #####
################################################################################

def tile_SVM_utr5(segment_chain,count_vector,trained_SVM, scale_logical, scaler_function):
    """Construct Ingolia vectors (by calling construct_Ingolia_vector) for all positions in 5'UTR of input segment chain, predicting whether each site is a start codon using a trained SVM and returning only the positive hits as a list
    
    --Input--
    segment_chain: plastid 'transcript' object to be used for analysis
    count_vector: vector of counts in harringtonine dataset at all positions in segment_chain (segment_chain.get_counts(BAM array Harring))
    trained_SVM: an SVC object (from sklearn.svm) which has been trained on annotated CDS as per Ingolia
    scale_logical: boolean, should the ingolia vector be scaled (ie. if the SVM was trained on a scaled array)
    scaler_function: function of type preprocessing.StandardScaler().fit(training_array) for given training_array. Required if the SVM was trained on a scaled array (ie. if scale_logical=True)
    
    --Output--
    List of nt positions (in transcript coordinates) where Ingolia vector was classified as 'positive' by the trained_SVM
    If no positive positions or a UTR <7nt, returns an empty list [] 
    """
    #
    positive_call=[]
    if segment_chain.get_utr5().get_length() < 7:
        return positive_call
    #
    for z in range(7,segment_chain.cds_start-1):
        if (z+40 < segment_chain.get_length()):
            ingolia_vector=np.asarray(construct_Ingolia_vector(z,count_vector))
            if scale_logical:
                ingolia_vector = ingolia_vector.reshape(1, -1)
                ingolia_vector = scaler_function.transform(ingolia_vector)
                if trained_SVM.predict(ingolia_vector)[0]==1:
                    positive_call.append(z)
                #
            #
        elif trained_SVM.predict(ingolia_vector.reshape(1,-1))[0]==1:
                positive_call.append(z)
            #
        #
    #
    return positive_call

def find_start_in_peak(peak_range, transcript_sequence, count_vector, reads_cutoff, canonical_start_codon, nearcanonical_start_codon):
    """Takes a harringtonine peak, defined as a list of nt positions (converted to np.array internally) predicted by SVM. Implements peak QC filtering before looking for a start codon and returning a segmentchain spanning the identified ORF. Canonical start codons are given preference. 
    
    --Input--
    peak_range: list of nt positions (transcript coordinates) passing SVM prediction
    transcript_sequence: nt sequence of the transcript
    count_vector: array of counts for transcript of interest
    reads_cutoff: minimum number of reads in the peak's 'vecotor window' (-7, +40 relative to the peak boundaries) for QC filtering
    canonical_start_codon, nearcanonical_start_codon, stop_codon: regex (specified as re.compile(regex)) for indicated codons
    **note canonical start codons are given preference in the script.
    
    --Output--
    Integer of type 'numpy.int64' containing first nt of start codon, if start codon can be found in peak
    If no start codon can be found, returns type(str) with explanation.
    
    """
    
    #Convert Peak to Numpy Array for Better Performance
    range_numpy=np.asarray(peak_range)
    #Initial QC
    if np.nansum(count_vector[min(range_numpy)-7:max(range_numpy)+40]) < reads_cutoff:
        return 'none. failed QC'
    
    search_space_sequence=transcript_sequence[min(range_numpy)-7:max(range_numpy)+40]
    
    #Find Canonical Starts, restricted to the peak boundaries. Convert to numpy array
    possible_canonical_starts=[]
    for location in canonical_start_codon.finditer(search_space_sequence):
        possible_canonical_starts.append(location.start())
    
    possible_canonical_starts=np.asarray(possible_canonical_starts)
    possible_canonical_starts=possible_canonical_starts + min(range_numpy) - 7 #starts are relative to search space. We want transcript coords
    
    #Check if there are canonical starts. 
    if len(possible_canonical_starts)>0:
        #If so, find the most likely, assuming this is closest to the center of the harringtonine peak
        likely_start = possible_canonical_starts[np.argmin(np.absolute(possible_canonical_starts-(sum(peak_range)/len(peak_range))))]
    
    else:
        #If not, check for non-canonical restricted to peak boundaries
        possible_noncanonical_starts=[]
        for location in nearcanonical_start_codon.finditer(search_space_sequence):
            possible_noncanonical_starts.append(location.start())
        
        possible_noncanonical_starts=np.asarray(possible_noncanonical_starts)
        possible_noncanonical_starts=possible_noncanonical_starts + min(range_numpy) - 7 #starts are relative to search space. We want transcript coords
        
        if len(possible_noncanonical_starts)>0:
            likely_start = possible_noncanonical_starts[np.argmin(np.absolute(possible_noncanonical_starts-(sum(peak_range)/len(peak_range))))]
        
        else:
            likely_start = 'none'
        #
    #
    return likely_start

def pair_start_with_stop(start_codon_array, transcript_sequence, stop_codon_regex):
    """Takes a list of start codons for a given sequence and pairs with first in-frame stop codon. Does NOT check that this is not the CDS_end for the sake of flexiability (preserves ability to call N-terminal extensions). 
    
    --Input--
    start_codon_array: array (internally coerced to np.array) containing start codons identified in a sequence
    transcript_sequence: nt sequence for which ORFs are meant to be identified. Use Plastid method .get_sequence()
    stop_codon_regex: regex (re.compile) to identify stop codons
    
    --Output--
    two lists, the first containing start codons with identified in frame stops, and the second containing those paired stop codons
    
    """
    stops = []
    for x in stop_codon_regex.finditer(transcript_sequence):
        stops.append(x.start())
    
    #Convert to np.array
    starts = np.asarray(start_codon_array)
    stops = np.asarray(stops)
    
    filtered_starts=[]
    filtered_stops=[]
    for identified_start in starts:
        codon_ct = (stops - identified_start)/3 
        codon_ct = codon_ct[codon_ct>0]
        orfs = [a for a in codon_ct if a.is_integer()] #gives a list but thats fine
        
        if len(orfs)>0:
            final_orf_codons = min(orfs)
            predicted_stop = identified_start + final_orf_codons.astype(int)*3 + 3
            filtered_starts.append(identified_start)
            filtered_stops.append(predicted_stop)
        #
    #
    return filtered_starts, filtered_stops

def assign_uORFs_from_harr_peaks(input_transcript, path_to_harringtonine_reads, mapping_for_harringtonine_reads, trained_SVM, scale_logical, scaler_function, genome_twobit, canonical_start_codon, nearcanonical_start_codon, stop_codon, reads_cutoff):
    """Wrapper function for uORF assigment from harringtonine peak data. Workflow as follows:
        (1) Calls tile_SVM_utr5 to create Ingolia vectors for all positions within the 5'UTR of the input transcript and classify using trained SVM. Aggregates positive nt locations into peaks. 
        (2) Loops through peaks and calls find_start_in_peak to annotate canonical or non-canonical start codons
        (3) Loops through UNIQUE start codons identified in peaks and pairs with stop codon by calling pair_start_with_stop
        (4) Loops through start/stop uORF pairs and extracts uORFs as plastid segmentchains, so long as the stop codon is not equal to the cds_stop for that transcript (avoids calling CDS and N-terminal extensions of CDS as uORFs)
        
    --Input--
    input_transcript: plastid transcript object
    path_to_harringtonine_reads: path to bam file for harringtonine reads. Used to make BAMGenomeArray containing Harringtonine read aligments
    mapping_for_harringtonine_reads: path to psite offset file generated by psite script, applies mapping to BAMGenomeArray. 
    trained_SVM: SVM trained to call start_codons based on Ingolia vector
    scale_logical: passed to tile_SVM_utr5. Was the SVM trained on a scaled training set? If true, you must provide a scaler function to transform the generated arrays prior to classification
    scaler_function: passed to tile_SVM_utr5. Function of type preprocessing.StandardScaler().fit(training_array) for given training_array.
    genome_twobit: twobit genome file
    canonical_start_codon: regular expression (re.compile) to find start codons, as required by find_start_in_peak
    nearcanonical_start_codon: regular expression (re.compile) to find near-canonical starts, which will be searched for in peak if a canonical start cannot be found using find_start_in_peak
    stop_codon: regex (re.compile) for stop codons, passed to pair_start_with_stop internally
    reads_cutoff: passed to find_start_in_peak. Used for initial QC of discovered peaks, requring that a peak has > this number of reads in the vector range (-7 to +40 relative to peak bounds)
    
    --Output--
    A list with found uORFs as plastid segmentchains, named as input_transcript.get_name() + start codon location. If no uORFs are found, an empty list.
    """
    print 'Working on ' + input_transcript.get_name() + ' ...'
    
    #Generate Harringtonine Read BAMGenomeArray Interally, as Object cannot otherwise be pickled.
    harringtonine_reads = BAMGenomeArray(path_to_harringtonine_reads)
    harringtonine_reads.set_mapping(VariableFivePrimeMapFactory.from_file(open(mapping_for_harringtonine_reads)))
    
    #Generate the count vector object used to construct Ingolia vectors
    count_vector=input_transcript.get_counts(harringtonine_reads)
    
    #Tile across all 5'UTR and construct vectors at each position, testing with SVM and storing only if predicted start
    predicted_starts=tile_SVM_utr5(input_transcript,count_vector,trained_SVM, scale_logical, scaler_function)
    
    print 'Found ' + str(len(predicted_starts)) + ' positive positions'
    if len(predicted_starts)==0:
        print '...Done!'
        return []
    
    #Concatenate positive positions into peaks 
    positive_ranges=[list(group) for group in mit.consecutive_groups(predicted_starts)]
    print 'Concatenated positions to ' + str(len(positive_ranges)) + ' peak(s)'
    
    #Get Sequence of transcript
    transcript_sequence=input_transcript.get_sequence(genome_twobit)
    
    #Loop through peaks and find start codons
    called_start = []
    for peak in positive_ranges:
        identified_start = find_start_in_peak(peak_range=peak, transcript_sequence=transcript_sequence, count_vector=count_vector, reads_cutoff=reads_cutoff, canonical_start_codon=canonical_start_codon, nearcanonical_start_codon=nearcanonical_start_codon) 
        
        #Append Found Starts
        if type(identified_start) is not str:
            called_start.append(identified_start)
    
    #Find only unique start codons
    called_start=np.asarray(called_start)
    called_start=np.unique(called_start)
    
    if len(called_start)==0:
        print 'Identified 0 start codons.'
        print '...Done!'
        return []
    
    print 'Identified ' + str(len(called_start)) + ' unique start codon(s)'
    
    #Identify uORFs
    orf_start, orf_stop = pair_start_with_stop(start_codon_array=called_start,transcript_sequence=transcript_sequence,stop_codon_regex=stop_codon)
    
    if len(orf_start) == 0 or len(orf_stop)==0:
        print 'Paired 0 start codons.'
        print '...Done!'
        return []
    
    print 'Identified ' + str(len(orf_start)) + ' uORF(s)'
    
    #Probably do not need to look for unique stops because we are predicting peaks based on real data, not computationally where you worry about calling in-frame methionines as new start codons. 
    
    #Extract uORF Sequences as segment chains
    count=0
    segmentchain_list = []
    for start,stop in itertools.izip(orf_start,orf_stop):
        if stop != input_transcript.cds_end:
            discovered_subchain = input_transcript.get_subchain(start,stop) #Cannot simply do my_transcript.get_subchain(start,stop,'ID'='x') b/c hardcoded that ID of subchain = ID transcript + 'subchain'. This apparently cannot be overwritten
            discovered_subchain.attr['ID'] = input_transcript.get_name() + '_' + str(start) #overwrite the attribute to identify uORF uniquely. START this time since we are identifying based on unique start peak and allowing multiple uORF to share the same stop.
            segmentchain_list.append(discovered_subchain)
            count=count+1
        #
    #
    print str(count) + ' uORF(s) passed QC and were appended to list'
    print '...Done!'
    return segmentchain_list

#########################################################
#### Script to Find uORFs in  the HEK293T Cell Data  ####
#########################################################

#Get Everything Set up
#hg38_transcripts = list(GTF2_TranscriptAssembler('UCSC_Gencode_hg38.gtf'))
#dill.dump(hg38_transcripts, open('hg38_plastid_transcripts.obj','wb'))
hg38_transcripts = dill.load(open('hg38_plastid_transcripts.obj'))
hg38_genome = twobit.TwoBitFile('file.path')

#Count reads over each transcript
CHXcounts = count_CHXreads_to_dataframe(list_of_transcripts=hg38_transcripts, path_to_CHX_reads='file.path', path_to_CHX_psite='file.path', nthreads=23)
CHXcounts['name'] = CHXcounts.segmentchain.apply(lambda x: x.get_name())
CHXcounts.iloc[:,1:].to_csv(path_or_buf = 'cds_translation_HEK.tsv',sep='\t',header=True,index=False) #analyzed in R. 

np.nanpercentile(CHXcounts.rpnt, 90)
#Define highly_tranlsated as >2.512 reads per nt in CDS. This is the 90th percentile for transcripts with a CDS (removed Na's) This captures 10,669 transcripts which is 5% of input dataset, 10% of the transcripts with a CDS

highly_translated = CHXcounts.loc[CHXcounts.rpnt > np.nanpercentile(CHXcounts.rpnt,90)].iloc[:,0].tolist()
#dill.dump(highly_translated, open('hg38_highlytranslated_transcripts.obj','wb'))
#highly_translated = dill.load(open("hg38_highlytranslated_transcripts.obj"))

#Train the SVM
#Generate arrays on WHOLE dataset (GridSearchCV will do the splitting for you during cross-validation)
codonvectors_grid, assignments_grid = generate_SVM_arrays_around_cds_start(list_input_transcripts=highly_translated, path_to_harringtonine_reads='file.path', path_to_harringtonine_psite='file.path', nthreads=23)

#Scale the Input Arrays and retain the scaler
scaler_grid = preprocessing.StandardScaler().fit(codonvectors_grid)
codonvectors_grid_scale = scaler_grid.transform(codonvectors_grid)

#Cross Validation Approach to Train the SVM. Use similar spread of parameters.
parameters = {'C':[0.1,1,2,4,10],'gamma':[0.001,0.01,0.1,1,2,2.4,3,5, 10], 'class_weight':[{0:1,1:10}, {0:1,1:5}, {0:1,1:4}, {0:1,1:3}, {0:1,1:2}, {0:1,1:1}]}
clf_test = SVC(kernel='rbf', cache_size=1000) 
cv_SVM_human = GridSearchCV(clf_test, parameters, n_jobs=23, refit=True) #note that we run this over the whole genome
cv_SVM_human.fit(codonvectors_grid_scale, assignments_grid)
cv_SVM_human.best_params_ #Wants C=10, gama 0.01, weight 0:1, 1:3
cv_SVM_human.best_score_ #95% performance
#Find that with setting refit=True for GridSearchCV, can just use test object here to predict (can call test.predict() so it should work)
#dill.dump(scaler_grid, open('hg38_HEK_scaler.obj','wb'))
#dill.dump(cv_SVM_human, open('hg38_HEK_SVM.obj','wb'))
#scaler_grid = dill.load(open('hg38_HEK_scaler.obj'))
#cv_SVM_human = dill.load(open('hg38_HEK_SVM.obj'))

#Compile Regex for Ranges
canonical_start_codon = re.compile('ATG', re.IGNORECASE)
nearcanonical_start_codon = re.compile('(CTG)|(GTG)|(TTG)|(ACG)|(AGG)|(ATC)|(ATT)|(AAG)|(ATA)', re.IGNORECASE) #figure out non-canonicals.
stop_codon = re.compile('(TAA)|(TAG)|(TGA)', re.IGNORECASE)

#Run Main Script. Note that because we have used refit=True with GridSearchCV we can use the SVM directly in the function (which just requires that the classifier has a .predict() method.)
annotated_ORFs = Pool(23).map(assign_uORFs_from_harr_peaks, highly_translated, itertools.repeat('file.path'), itertools.repeat('file.path'), itertools.repeat(cv_SVM_human), itertools.repeat(True), itertools.repeat(scaler_grid), itertools.repeat(hg38_genome), itertools.repeat(canonical_start_codon), itertools.repeat(nearcanonical_start_codon), itertools.repeat(stop_codon), itertools.repeat(50))

#Reformat uORF annotations in a logical way. Start here if coming back from running a model with known settings. 
annotated_ORFs_final = [i for i in annotated_ORFs if len(i)>0]
annotated_ORFs_final = list(itertools.chain.from_iterable(annotated_ORFs_final))
#dill.dump(annotated_ORFs_final, open('HEK293T_uORFs.obj','wb'))
#annotated_ORFs_final = dill.load(open('HEK293T_uORFs.obj'))

#Have a look at the data first to see how we did. Looks fine. Definitely fewer 5'UTR reads and fewer uORFs. Cell culture probs? 
fout = open('HEK293T_uORFs.bed','w')
for i in annotated_ORFs_final:
    fout.write(i.as_bed())

fout.close()

##################################################
#### Generate files for Everyone's Analysis   ####
##################################################
#Check that uORF names are unique. Named based on start codon. 
uORF_names = [i.get_name() for i in annotated_ORFs_final]
len(np.unique(uORF_names)) == len(uORF_names)

#Check that transcript names are unique. Remember we analyzed highly_translated (NOT all the transcripts...ie. 200k lol) so this is the set to use. 
names_transcripts = [i.get_name() for i in highly_translated]
len(np.unique(names_transcripts)) == len(names_transcripts) #we are good 

#Write fasta files for uORFs
fout = open('./output_for_model/HEK293T_uORFs.fasta','w')
for i in annotated_ORFs_final:
    fout.write(i.get_fasta(hg38_genome))

fout.close()

#Write fasta files for CDS
fout = open('./output_for_model/HEK293T_CDS.fasta','w')
for i in highly_translated:
    fout.write(i.get_cds().get_fasta(hg38_genome))

fout.close()

#Create Full Transcript dictionary: key = transcript name, dictionary = transcript object. Necessary for lookup. 
full_transcript_dict = {transcript.get_name(): transcript for transcript in highly_translated}

#Find -50 to +50 around uORF start codons
windows_list = []
errors = []
extract_start = re.compile(".*?_([0-9]+)")
for index, i in enumerate(annotated_ORFs_final):
    print 'Working on ' + i.get_name()
    print 'Index is: ' + str(index)
    
    #Extract Full Transcript Object from full_transcript_dict
    full_transcript = full_transcript_dict[i.attr['transcript_id']]
    
    #First Extract the position of the uORF_start. This is in transcript coordinates remember.
    start_position_match = extract_start.match(i.get_name())
    start_position = int(start_position_match.groups()[0])
    
    #Check uORF was correctly identified
    if i.get_sequence(hg38_genome) != full_transcript.get_sequence(hg38_genome)[start_position:start_position + i.get_length()]:
        errors.append(i)
    
    #Determine lower and upper bounds based on start position    
    if start_position - 50 >= 0:
        lower_bound = start_position - 50
    #
    else:
        lower_bound = 0 
    #
    if start_position + 50 <= full_transcript.get_length():
        upper_bound = start_position + 50
    #
    else:
        upper_bound = full_transcript.get_length()
    #
    subchain_window = full_transcript.get_subchain(lower_bound, upper_bound)
    subchain_window.attr['ID'] = i.get_name() + '_' + str('100bpwindow') + '_' + str(start_position-lower_bound) #overwrite the attribute to identify uORF uniquely
    windows_list.append(subchain_window)

#Write uORF_windows output as fasta 
fout=open('./output_for_model/HEK293T_uORFs_100bpwindows_aroundstart.fasta','w')
for i in windows_list:
    fout.write(i.get_fasta(hg38_genome))

fout.close()

#Find -50 to +50 windows around CDS_start
cds_windows_list = []
for index, i in enumerate(highly_translated):
    print 'Working on ' + i.get_name()
    print 'Index is: ' + str(index)
    
    #Get full transcript (already have it) and cds_start
    full_transcript = i #just for clarity, remember you don't need a dictionary here b/c it is just annotated CDS_start
    start_position = full_transcript.cds_start
    
    if start_position - 50 >= 0:
        lower_bound = start_position - 50
    #
    else:
        lower_bound = 0 
    #
    if start_position + 50 <= full_transcript.get_length():
        upper_bound = start_position + 50
    #
    else:
        upper_bound = full_transcript.get_length()
    #
    subchain_window = full_transcript.get_subchain(lower_bound, upper_bound)
    subchain_window.attr['ID'] = full_transcript.attr['transcript_id'] + '_' + str('100bpwindow') + '_' + str(start_position-lower_bound) #overwrite the attribute to identify uORF uniquely
    cds_windows_list.append(subchain_window)

#Write CDS windows output as fasta
fout=open('./output_for_model/HEK293T_CDS_100bpwindows_aroundstart.fasta','w')
for i in cds_windows_list:
    fout.write(i.get_fasta(hg38_genome))

fout.close()

#Folks wanted the Distance of uORF to CDS
distance_table = pd.DataFrame(index=range(0,len(annotated_ORFs_final)),columns=['uORF_ID', 'dist_uORFstart_to_CDS', 'dist_uORFend_to_CDS', 'Len_uORF'])
extract_start = re.compile(".*?_([0-9]+)") #re-use this to find uORF start which I tucked into the ID. Defined again for clarity! 
for index, feature in enumerate(annotated_ORFs_final):
    print 'Working on: ' + feature.get_name()
    print 'Index is: ' + str(index)
    distance_table.uORF_ID[index] = feature.get_name()
    
    #Extract start and stop for uORF in Transcript Coordinates
    start_position_match = extract_start.match(feature.get_name())
    start_position = int(start_position_match.groups()[0])
    stop_position = start_position + feature.get_length()
    
    #Obtain full transcript annotation, which contains cds_start location
    full_transcript = full_transcript_dict[feature.attr['transcript_id']]
    
    #Calculate Distances and save to dataframe
    distance_table.dist_uORFstart_to_CDS[index] = full_transcript.cds_start - start_position
    distance_table.dist_uORFend_to_CDS[index] = full_transcript.cds_start - stop_position
    distance_table.Len_uORF[index] = feature.get_length()

distance_table.to_csv(path_or_buf='./output_for_model/HEK293T_uORFs_distance_toCDSstart.tsv', sep='\t',header=True, index=False, index_label=False)

#Count CHX Reads Across all the uORFs
CHX_reads_array = BAMGenomeArray('file.path')
CHX_reads_array.set_mapping(VariableFivePrimeMapFactory.from_file(open('file.path')))

uORF_CHX_counts = pd.DataFrame(index=range(0, len(annotated_ORFs_final)), columns = ['uORF_ID', 'transcript', 'Length_uORF', 'uORF_CHX_counts']) 
for index, feature in enumerate(annotated_ORFs_final):
    print 'Working on: ' + feature.get_name()
    print 'Index is: ' + str(index)
    uORF_CHX_counts.uORF_ID[index] = feature.get_name()
    uORF_CHX_counts.transcript[index] = feature.attr['transcript_id']
    uORF_CHX_counts.Length_uORF[index] = feature.get_length()
    uORF_CHX_counts.uORF_CHX_counts[index] = np.nansum(feature.get_counts(CHX_reads_array))

uORF_CHX_counts.to_csv(path_or_buf='./output_for_model/HEK293T_uORF_CHXcounts.tsv', sep='\t', index=False, header=True)

#Count rnaseq and ribosome profiling reads across all the CDS. 
rnaseq_reads = BAMGenomeArray('file.path')
rnaseq_reads.set_mapping(CenterMapFactory())

CDS_counts_table = pd.DataFrame(index=range(0,len(highly_translated)), columns=['transcript', 'Length_CDS', 'CDS_CHX_Counts', 'CDS_RNAseq_Counts'])
for index, feature in enumerate(highly_translated):
    print 'Working on: ' + feature.get_name()
    print 'Index is: ' + str(index)
    feature_CDS = feature.get_cds()
    CDS_counts_table.transcript[index] = feature_CDS.attr['transcript_id']
    CDS_counts_table.Length_CDS[index] = feature_CDS.get_length()
    CDS_counts_table.CDS_CHX_Counts[index] = np.nansum(feature_CDS.get_counts(CHX_reads_array))
    CDS_counts_table.CDS_RNAseq_Counts[index] = np.nansum(feature_CDS.get_counts(rnaseq_reads))

CDS_counts_table.to_csv(path_or_buf = './output_for_model/HEK293T_CDS_CHX_and_mRNA_counts.tsv', sep='\t',index=False,header=True)